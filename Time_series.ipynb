{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c45220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, classification_report, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c84d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/25day_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06b0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in range(len(data)):\n",
    "    if data['wahing_machine'].iloc[i]==0 and data['dishwasher'].iloc[i]==0 and data['oven'].iloc[i]==0:\n",
    "        label.append(0)\n",
    "    elif data['wahing_machine'].iloc[i]>0:\n",
    "        label.append(1)\n",
    "    elif data['dishwasher'].iloc[i]>0:\n",
    "        label.append(2)\n",
    "    elif data['oven'].iloc[i]>0:\n",
    "        label.append(3)\n",
    "data['Class'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95695858",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_set1 =data[data['DateTime'].str.startswith('2022-01-01')]\n",
    "Test_set2 =data[data['DateTime'].str.startswith('2022-01-02')]\n",
    "Test_set3 =data[data['DateTime'].str.startswith('2022-01-03')]\n",
    "Test_set4 =data[data['DateTime'].str.startswith('2022-01-04')]\n",
    "Test_set5 =data[data['DateTime'].str.startswith('2022-01-05')]\n",
    "Test_set6 =data[data['DateTime'].str.startswith('2022-01-06')]\n",
    "Test_set7 =data[data['DateTime'].str.startswith('2022-01-07')]\n",
    "Test_set8 =data[(data['DateTime'].str.startswith('2022-01-08')) | (data['DateTime'].str.startswith('2022-01-09'))]\n",
    "Test_set9 =data[(data['DateTime'].str.startswith('2022-01-10')) | (data['DateTime'].str.startswith('2022-01-11'))]\n",
    "Test_set10 =data[(data['DateTime'].str.startswith('2022-01-12')) | (data['DateTime'].str.startswith('2022-01-13'))]\n",
    "test_set_list= [Test_set1, Test_set2, Test_set3, Test_set4, Test_set5, Test_set6, Test_set7, Test_set8, Test_set9, Test_set10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7b2bf",
   "metadata": {},
   "source": [
    "Crea una copia del dataset e ripuliscilo dai dati presenti nelle fold. Utile per addestramenti esterni. Per addestramenti interni(quindi uno per fold bisogna eliminare solo il Test_set corrente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11b8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = data.copy()\n",
    "for i in range(len(test_set_list)):\n",
    "    dfi.drop(test_set_list[i].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa119b",
   "metadata": {},
   "source": [
    "Training del modello esternamente alle fold facendo una compressione stile Binning per le time series. Vengono presi 5 campioni consecutivi e si fa la media dei valori comprimendoli in uno solo. Per la colonna della classe si usa una moda in modo da scegliere quella piu rappresenta fra i 5 campioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60038e27",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m Training_set \u001b[38;5;241m=\u001b[39m dfi\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m Training_set\u001b[38;5;241m.\u001b[39mgroupby(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(dfi))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m----> 4\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mTraining_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdfi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m X\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwahing_machine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdishwasher\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moven\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m y\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mClass\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:900\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    898\u001b[0m gba \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, [func], args\u001b[38;5;241m=\u001b[39m(), kwargs\u001b[38;5;241m=\u001b[39m{})\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 900\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno results\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# raised directly by _aggregate_multiple_funcs\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:171\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_dict_like()\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(arg):\n\u001b[0;32m    174\u001b[0m     f \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mget_cython_func(arg)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:375\u001b[0m, in \u001b[0;36mApply.agg_list_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# Capture and suppress any warnings emitted by us in the call\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;66;03m# to agg below, but pass through any warnings that were\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# generated otherwise.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;66;03m# This is necessary because of https://bugs.python.org/issue29672\u001b[39;00m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;66;03m# See GH #43741 for more details\u001b[39;00m\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m record:\n\u001b[1;32m--> 375\u001b[0m         new_res \u001b[38;5;241m=\u001b[39m \u001b[43mcolg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(record) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    377\u001b[0m         match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(depr_nuisance_columns_msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:271\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, abc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m--> 271\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_multiple_funcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;66;03m# \"Optional[List[str]]\", variable has type \"Index\")\u001b[39;00m\n\u001b[0;32m    275\u001b[0m         ret\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m columns  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:326\u001b[0m, in \u001b[0;36mSeriesGroupBy._aggregate_multiple_funcs\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arg):\n\u001b[0;32m    325\u001b[0m     key \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mOutputKey(label\u001b[38;5;241m=\u001b[39mname, position\u001b[38;5;241m=\u001b[39midx)\n\u001b[1;32m--> 326\u001b[0m     results[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:287\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# TODO: KeyError is raised in _python_agg_general,\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;66;03m#  see test_groupby.test_basic\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_named(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1481\u001b[0m, in \u001b[0;36mGroupBy._python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;66;03m# if this function is invalid for this dtype, we will ignore it.\u001b[39;00m\n\u001b[1;32m-> 1481\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1483\u001b[0m     warn_dropping_nuisance_columns_deprecated(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:981\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    978\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 981\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1005\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m   1004\u001b[0m     group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39m__finalize__(obj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1005\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     res \u001b[38;5;241m=\u001b[39m libreduction\u001b[38;5;241m.\u001b[39mextract_result(res)\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1467\u001b[0m, in \u001b[0;36mGroupBy._python_agg_general.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_agg_general\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1466\u001b[0m     func \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mis_builtin_func(func)\n\u001b[1;32m-> 1467\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# iterate through \"columns\" ex exclusions to populate output dict\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     output: \u001b[38;5;28mdict\u001b[39m[base\u001b[38;5;241m.\u001b[39mOutputKey, ArrayLike] \u001b[38;5;241m=\u001b[39m {}\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      2\u001b[0m Training_set \u001b[38;5;241m=\u001b[39m dfi\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m Training_set\u001b[38;5;241m.\u001b[39mgroupby(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(dfi))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m----> 4\u001b[0m y \u001b[38;5;241m=\u001b[39m Training_set\u001b[38;5;241m.\u001b[39mgroupby(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(dfi))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m X\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwahing_machine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdishwasher\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moven\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m y\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mClass\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py:970\u001b[0m, in \u001b[0;36mIndexOpsMixin.value_counts\u001b[1;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_counts\u001b[39m(\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    886\u001b[0m     normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    890\u001b[0m     dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Return a Series containing counts of unique values.\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:874\u001b[0m, in \u001b[0;36mvalue_counts\u001b[1;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[0;32m    871\u001b[0m         result \u001b[38;5;241m=\u001b[39m Series(counts, index\u001b[38;5;241m=\u001b[39mkeys, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort:\n\u001b[1;32m--> 874\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    877\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:3526\u001b[0m, in \u001b[0;36mSeries.sort_values\u001b[1;34m(self, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   3524\u001b[0m \u001b[38;5;66;03m# GH 35922. Make sorting stable by leveraging nargsort\u001b[39;00m\n\u001b[0;32m   3525\u001b[0m values_to_sort \u001b[38;5;241m=\u001b[39m ensure_key_mapped(\u001b[38;5;28mself\u001b[39m, key)\u001b[38;5;241m.\u001b[39m_values \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 3526\u001b[0m sorted_index \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_to_sort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mascending\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3528\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(\n\u001b[0;32m   3529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[sorted_index], index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[sorted_index]\n\u001b[0;32m   3530\u001b[0m )\n\u001b[0;32m   3532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\sorting.py:423\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Finally, place the NaNs at the end or the beginning according to\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# na_position\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 423\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m na_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    425\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([nan_idx, indexer])\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Binning\n",
    "Training_set = dfi.copy()\n",
    "X = Training_set.groupby(np.arange(len(dfi))//5).mean()\n",
    "y = Training_set.groupby(np.arange(len(dfi))//5).agg(lambda x:x.value_counts().index[0])\n",
    "X= X.drop(['DateTime', 'Class', 'wahing_machine', 'dishwasher', 'oven'], axis=1)\n",
    "y= y.Class\n",
    "    \n",
    "#Undersampling per bilanciare il dataset\n",
    "two= int((y.values == 2).sum())\n",
    "three= int((y.values == 3).sum())\n",
    "undersample = RandomUnderSampler(sampling_strategy={0: 15000, 1: 15000, 2: two, 3: three})\n",
    "X_new, y_new = undersample.fit_resample(X, y)\n",
    "    \n",
    "#Modello del classificatore\n",
    "clf = RandomForestClassifier(class_weight= {0: 5, 1: 5, 2: 1, 3: 1},\n",
    "                                criterion= 'entropy', \n",
    "                                max_depth= 25, \n",
    "                                max_features= 'sqrt',\n",
    "                                max_samples= 0.5,\n",
    "                                n_estimators= 50)    \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Calcolo f1_score e recall per ogni fold\n",
    "f_score={}\n",
    "recall={}\n",
    "for i in range(len(test_set_list)):\n",
    "    X_test = test_set_list[i].copy().drop(['DateTime', 'Class', 'wahing_machine', 'dishwasher', 'oven', 'ReactivePower', 'harmonic'], axis=1)\n",
    "    y_preds = model.predict(X_test)\n",
    "    y_test = test_set_list[i].Class\n",
    "    print(\"CM:\\n\" + str(confusion_matrix(y_test,y_preds)) + \"\\n\")\n",
    "    print(classification_report(y_test,y_preds))\n",
    "    r = recall_score(y_test,y_preds, average= None) \n",
    "    f = f1_score(y_test,y_preds, average= None)\n",
    "    f_score[i] = f\n",
    "    recall[i] = r \n",
    "\n",
    "#Calcolo le medie delle metriche di prestazione\n",
    "d= pd.DataFrame.from_dict(f_score, orient='index').replace(0, np.nan)\n",
    "print(d.mean(axis=0, skipna=True))\n",
    "d= pd.DataFrame.from_dict(recall, orient='index').replace(0, np.nan)\n",
    "print(d.mean(axis=0, skipna=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9c518",
   "metadata": {},
   "source": [
    "Funzione per il calcolo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c026c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start cleaning\n",
      "stop cleaning\n",
      "CM:\n",
      "[[116958    334     11      3]\n",
      " [   460   3916      0      1]\n",
      " [    76      0   1554      1]\n",
      " [    64      4      2   1032]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    117306\n",
      "           1       0.92      0.89      0.91      4377\n",
      "           2       0.99      0.95      0.97      1631\n",
      "           3       1.00      0.94      0.96      1102\n",
      "\n",
      "    accuracy                           0.99    124416\n",
      "   macro avg       0.98      0.95      0.96    124416\n",
      "weighted avg       0.99      0.99      0.99    124416\n",
      "\n",
      "CM:\n",
      "[[81580    80     2     3]\n",
      " [  317  3736     0     0]\n",
      " [    0     0     0     0]\n",
      " [  143     0     0   539]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     81665\n",
      "           1       0.98      0.92      0.95      4053\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.99      0.79      0.88       682\n",
      "\n",
      "    accuracy                           0.99     86400\n",
      "   macro avg       0.74      0.68      0.71     86400\n",
      "weighted avg       0.99      0.99      0.99     86400\n",
      "\n",
      "start cleaning\n",
      "stop cleaning\n",
      "CM:\n",
      "[[116958    337      5      6]\n",
      " [   427   3922      0      2]\n",
      " [    80      0   1597      0]\n",
      " [    68      2      1   1011]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    117306\n",
      "           1       0.92      0.90      0.91      4351\n",
      "           2       1.00      0.95      0.97      1677\n",
      "           3       0.99      0.93      0.96      1082\n",
      "\n",
      "    accuracy                           0.99    124416\n",
      "   macro avg       0.98      0.95      0.96    124416\n",
      "weighted avg       0.99      0.99      0.99    124416\n",
      "\n",
      "CM:\n",
      "[[77707   555     3     4]\n",
      " [ 1321  5687     0     0]\n",
      " [    0     0     0     0]\n",
      " [   79     0     0  1044]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     78269\n",
      "           1       0.91      0.81      0.86      7008\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       1.00      0.93      0.96      1123\n",
      "\n",
      "    accuracy                           0.98     86400\n",
      "   macro avg       0.72      0.68      0.70     86400\n",
      "weighted avg       0.98      0.98      0.98     86400\n",
      "\n",
      "start cleaning\n",
      "stop cleaning\n",
      "CM:\n",
      "[[116710    345      4      7]\n",
      " [   524   4214      0      0]\n",
      " [    79      0   1419      0]\n",
      " [    84      1      0   1029]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    117066\n",
      "           1       0.92      0.89      0.91      4738\n",
      "           2       1.00      0.95      0.97      1498\n",
      "           3       0.99      0.92      0.96      1114\n",
      "\n",
      "    accuracy                           0.99    124416\n",
      "   macro avg       0.98      0.94      0.96    124416\n",
      "weighted avg       0.99      0.99      0.99    124416\n",
      "\n",
      "CM:\n",
      "[[80939    76    64    16]\n",
      " [  217   807     0     0]\n",
      " [ 1159    37  2381     0]\n",
      " [    0     0     0   704]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     81095\n",
      "           1       0.88      0.79      0.83      1024\n",
      "           2       0.97      0.67      0.79      3577\n",
      "           3       0.98      1.00      0.99       704\n",
      "\n",
      "    accuracy                           0.98     86400\n",
      "   macro avg       0.95      0.86      0.90     86400\n",
      "weighted avg       0.98      0.98      0.98     86400\n",
      "\n",
      "start cleaning\n",
      "stop cleaning\n",
      "CM:\n",
      "[[116823    326     10      4]\n",
      " [   561   4104      0      5]\n",
      " [    88      0   1622      0]\n",
      " [    79      0      1    793]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    117163\n",
      "           1       0.93      0.88      0.90      4670\n",
      "           2       0.99      0.95      0.97      1710\n",
      "           3       0.99      0.91      0.95       873\n",
      "\n",
      "    accuracy                           0.99    124416\n",
      "   macro avg       0.98      0.93      0.95    124416\n",
      "weighted avg       0.99      0.99      0.99    124416\n",
      "\n",
      "CM:\n",
      "[[81746     2     0     7]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]\n",
      " [ 1692   822     2  2129]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     81755\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       1.00      0.46      0.63      4645\n",
      "\n",
      "    accuracy                           0.97     86400\n",
      "   macro avg       0.49      0.36      0.40     86400\n",
      "weighted avg       0.98      0.97      0.97     86400\n",
      "\n",
      "start cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop cleaning\n",
      "CM:\n",
      "[[116791    324      6      4]\n",
      " [   482   4001      0      1]\n",
      " [    90      1   1584      3]\n",
      " [    82      2      1   1044]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    117125\n",
      "           1       0.92      0.89      0.91      4484\n",
      "           2       1.00      0.94      0.97      1678\n",
      "           3       0.99      0.92      0.96      1129\n",
      "\n",
      "    accuracy                           0.99    124416\n",
      "   macro avg       0.98      0.94      0.96    124416\n",
      "weighted avg       0.99      0.99      0.99    124416\n",
      "\n",
      "CM:\n",
      "[[82668    60     4     3]\n",
      " [  367  3275     0    23]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     82735\n",
      "           1       0.98      0.89      0.94      3665\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99     86400\n",
      "   macro avg       0.49      0.47      0.48     86400\n",
      "weighted avg       1.00      0.99      0.99     86400\n",
      "\n",
      "start cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop cleaning\n",
      "CM:\n",
      "[[116754    360     14     10]\n",
      " [   470   4209      0      1]\n",
      " [    95      1   1357      0]\n",
      " [    82      0      0   1063]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    117138\n",
      "           1       0.92      0.90      0.91      4680\n",
      "           2       0.99      0.93      0.96      1453\n",
      "           3       0.99      0.93      0.96      1145\n",
      "\n",
      "    accuracy                           0.99    124416\n",
      "   macro avg       0.97      0.94      0.96    124416\n",
      "weighted avg       0.99      0.99      0.99    124416\n",
      "\n",
      "CM:\n",
      "[[82341    46    28     7]\n",
      " [    0     0     0     0]\n",
      " [ 1145    73  2760     0]\n",
      " [    0     0     0     0]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     82422\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.99      0.69      0.82      3978\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98     86400\n",
      "   macro avg       0.49      0.42      0.45     86400\n",
      "weighted avg       0.99      0.98      0.98     86400\n",
      "\n",
      "start cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop cleaning\n",
      "CM:\n",
      "[[116967    311     11      5]\n",
      " [   472   3955      0      0]\n",
      " [    88      0   1619      0]\n",
      " [    56      3      0    929]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    117294\n",
      "           1       0.93      0.89      0.91      4427\n",
      "           2       0.99      0.95      0.97      1707\n",
      "           3       0.99      0.94      0.97       988\n",
      "\n",
      "    accuracy                           0.99    124416\n",
      "   macro avg       0.98      0.94      0.96    124416\n",
      "weighted avg       0.99      0.99      0.99    124416\n",
      "\n",
      "CM:\n",
      "[[80661   280     6     9]\n",
      " [  572  2685     0     1]\n",
      " [    0     0     0     0]\n",
      " [  668     1     3  1514]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     80956\n",
      "           1       0.91      0.82      0.86      3258\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.99      0.69      0.82      2186\n",
      "\n",
      "    accuracy                           0.98     86400\n",
      "   macro avg       0.72      0.63      0.67     86400\n",
      "weighted avg       0.98      0.98      0.98     86400\n",
      "\n",
      "start cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop cleaning\n",
      "CM:\n",
      "[[112170    335      9      4]\n",
      " [   408   3839      0      1]\n",
      " [    51      1   1370      0]\n",
      " [    68      3      1    972]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    112518\n",
      "           1       0.92      0.90      0.91      4248\n",
      "           2       0.99      0.96      0.98      1422\n",
      "           3       0.99      0.93      0.96      1044\n",
      "\n",
      "    accuracy                           0.99    119232\n",
      "   macro avg       0.98      0.95      0.96    119232\n",
      "weighted avg       0.99      0.99      0.99    119232\n",
      "\n",
      "CM:\n",
      "[[160375    378    149     15]\n",
      " [  1206   6237      1      0]\n",
      " [  1244     42   2347      0]\n",
      " [   626      0      0    180]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    160917\n",
      "           1       0.94      0.84      0.88      7444\n",
      "           2       0.94      0.65      0.77      3633\n",
      "           3       0.92      0.22      0.36       806\n",
      "\n",
      "    accuracy                           0.98    172800\n",
      "   macro avg       0.95      0.68      0.75    172800\n",
      "weighted avg       0.98      0.98      0.98    172800\n",
      "\n",
      "start cleaning\n",
      "stop cleaning\n",
      "CM:\n",
      "[[111626    324      7      5]\n",
      " [   531   3972      0      2]\n",
      " [    94      2   1649      1]\n",
      " [    56      4      0    959]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    111962\n",
      "           1       0.92      0.88      0.90      4505\n",
      "           2       1.00      0.94      0.97      1746\n",
      "           3       0.99      0.94      0.97      1019\n",
      "\n",
      "    accuracy                           0.99    119232\n",
      "   macro avg       0.98      0.94      0.96    119232\n",
      "weighted avg       0.99      0.99      0.99    119232\n",
      "\n",
      "CM:\n",
      "[[168023    160     34     14]\n",
      " [   346   2444      0      0]\n",
      " [     0      0      0      0]\n",
      " [   773      0      0   1006]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    168231\n",
      "           1       0.94      0.88      0.91      2790\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.99      0.57      0.72      1779\n",
      "\n",
      "    accuracy                           0.99    172800\n",
      "   macro avg       0.73      0.61      0.66    172800\n",
      "weighted avg       0.99      0.99      0.99    172800\n",
      "\n",
      "start cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\massi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop cleaning\n",
      "CM:\n",
      "[[112240    298      7      6]\n",
      " [   475   3842      0      6]\n",
      " [    68      1   1344      1]\n",
      " [    82      4      1    857]]\n",
      "\n",
      "Test_set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    112551\n",
      "           1       0.93      0.89      0.91      4323\n",
      "           2       0.99      0.95      0.97      1414\n",
      "           3       0.99      0.91      0.94       944\n",
      "\n",
      "    accuracy                           0.99    119232\n",
      "   macro avg       0.98      0.94      0.95    119232\n",
      "weighted avg       0.99      0.99      0.99    119232\n",
      "\n",
      "CM:\n",
      "[[159666    184     56     16]\n",
      " [  1234   5256      0      0]\n",
      " [   721      0   2742      0]\n",
      " [   469      1      1   2454]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    159922\n",
      "           1       0.97      0.81      0.88      6490\n",
      "           2       0.98      0.79      0.88      3463\n",
      "           3       0.99      0.84      0.91      2925\n",
      "\n",
      "    accuracy                           0.98    172800\n",
      "   macro avg       0.98      0.86      0.91    172800\n",
      "weighted avg       0.98      0.98      0.98    172800\n",
      "\n",
      "0    0.992169\n",
      "1    0.888574\n",
      "2    0.812028\n",
      "3    0.782944\n",
      "dtype: float64\n",
      "0    0.997810\n",
      "1    0.845349\n",
      "2    0.699320\n",
      "3    0.687337\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "f_score={}\n",
    "recall={}\n",
    "for i in range(len(test_set_list)):\n",
    "    \n",
    "    print(\"start cleaning\")\n",
    "    dfi = data.drop(test_set_list[i].index)\n",
    "    y_= dfi.Class\n",
    "    X_= dfi.drop(['DateTime', 'Class', 'wahing_machine', 'dishwasher', 'oven'], axis=1)\n",
    "    X = X_.groupby(np.arange(len(dfi))//5).mean()\n",
    "    y = y_.groupby(np.arange(len(dfi))//5).agg(lambda x:x.value_counts().index[0])\n",
    "    print(\"stop cleaning\")\n",
    "       \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    two= int((y_train.values == 2).sum())\n",
    "    three= int((y_train.values == 3).sum())\n",
    "    undersample = RandomUnderSampler(sampling_strategy={0: 8000, 1: 8000, 2: two, 3: three})\n",
    "    X_new, y_new = undersample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    clf = RandomForestClassifier(class_weight= {0: 5, 1: 1, 2: 1, 3: 1},\n",
    "                                   criterion= 'entropy', \n",
    "                                   max_depth= 20,\n",
    "                                   max_features= 'sqrt',\n",
    "                                   max_samples= 0.4,\n",
    "                                   n_estimators= 25,\n",
    "                                )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_preds= clf.predict(X_test)\n",
    "    print(\"CM:\\n\" + str(confusion_matrix(y_test,y_preds)) + \"\\n\")\n",
    "    print(\"Test_set\\n\" + str(classification_report(y_test,y_preds)))\n",
    "    \n",
    "    X_test = test_set_list[i].copy().drop(['DateTime', 'Class', 'wahing_machine', 'dishwasher', 'oven'], axis=1)\n",
    "    y_preds = clf.predict(X_test)\n",
    "    y_test = test_set_list[i].Class\n",
    "    print(\"CM:\\n\" + str(confusion_matrix(y_test,y_preds)) + \"\\n\")\n",
    "    print(classification_report(y_test,y_preds))\n",
    "    r = recall_score(y_test,y_preds, average= None) \n",
    "    f = f1_score(y_test,y_preds, average= None)\n",
    "    f_score[i] = f\n",
    "    recall[i] = r \n",
    "    \n",
    "d= pd.DataFrame.from_dict(f_score, orient='index').replace(0, np.nan)\n",
    "print(d.mean(axis=0, skipna=True))\n",
    "d= pd.DataFrame.from_dict(recall, orient='index').replace(0, np.nan)\n",
    "d.mean(axis=0, skipna=True)\n",
    "print(d.mean(axis=0, skipna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b969e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
